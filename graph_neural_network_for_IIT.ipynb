{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### If any required libraries or packages fail to import, please ensure they are installed beforehand.\n",
    "import numpy as np\n",
    "import sys       \n",
    "import pandas as pd\n",
    "import datetime  \n",
    "import os     \n",
    "\n",
    "# This is necessary to avoid warnings about memory leaks in KMeans. Correct as needed.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"12\" \n",
    "\n",
    "import pickle\n",
    "import csv    \n",
    "import openpyxl \n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "from torch_geometric.utils import to_networkx \n",
    "\n",
    "import networkx as nx \n",
    "\n",
    "from lion_pytorch import Lion \n",
    "# LION optimizer\n",
    "# pip install lion-pytorch\n",
    "# cf. https://github.com/lucidrains/lion-pytorch\n",
    "\n",
    "from early_stopping import EarlyStopping\n",
    "# Early Stopping\n",
    "# cf. https://github.com/Bjarten/early-stopping-pytorch\n",
    "# get the file \"early_stopping.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialization_parameters_node_edge():\n",
    "    global feature_s_on, feature_degree_on, feature_prob_on, class_num, input_normalization\n",
    "    global out1_num, out1_items, out1_weight, out1_index\n",
    "    global inverse_T, closeness, betweenness, clustering\n",
    "    \n",
    "    # add each node state as a node feature    1:Yes    0:No\n",
    "    feature_s_on = 0       # fix this parameter as 0, do not change it\n",
    "    \n",
    "    # add node degrees as a node feature   1:Yes   0:No\n",
    "    feature_degree_on = 1\n",
    "\n",
    "    # add hihger probabilities as a node feature   2:Yes   0:No\n",
    "    feature_prob_on = 2    \n",
    "\n",
    "    # the number of classes in the classification task\n",
    "    class_num = 2      # fix this as 2 (label 'in' and 'out'), do not change it\n",
    "\n",
    "    # Input normalization  1:Yes  0:No\n",
    "    input_normalization = 1\n",
    "\n",
    "    # the number of scalar outputs (only big phi in this study)\n",
    "    out1_num = 1   # fix this as 1, do not change it\n",
    "    \n",
    "    # The following three parameters are utilized internally. Do not change them.\n",
    "    out1_items = ['BigPhi']\n",
    "    out1_weight = [1.0]\n",
    "    out1_index = ['Phi']    \n",
    "    \n",
    "    # add parameter T as a node feature   0:Yes   -1:No\n",
    "    inverse_T = 0        # do not worry about the name 'inverse'\n",
    "\n",
    "    # add closeness centrality as a node feature   1:Yes   0:No\n",
    "    closeness = 1\n",
    "\n",
    "    # add betweenness centrality as a node feature   1:Yes   0:No\n",
    "    betweenness = 1\n",
    "\n",
    "    # add clustering coefficients as a node feature   1:Yes   0:No\n",
    "    clustering = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialization_parameters_optimization():\n",
    "    global learning_rate, weight_decay_val, weights, patience_val, loss2_coef, optimizer_select\n",
    "    \n",
    "    learning_rate = 0.0001 \n",
    "\n",
    "    weight_decay_val = 0     # 0 in this study\n",
    "\n",
    "    weights = [1.8, 1.0]      # weights for class labels 'out' and 'in' \n",
    "\n",
    "    patience_val = 50    # patience in the early stopping strategy\n",
    "\n",
    "    loss2_coef = 5.0     # weight for the cross-entropy loss, 5 in this study\n",
    "\n",
    "    optimizer_select = 'Lion'    # select one among 'Lion', 'Adam', 'RAdam', and 'AdamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialization_parameters_GNN():\n",
    "    global convolution_type, other_network_type, drop_rate\n",
    "\n",
    "    convolution_type = 'Transformer'        # select one among 'Transformer', 'GraphConv', and 'GAT' \n",
    "    \n",
    "    # pooling type\n",
    "    # 0(proposed method):global_max, x - global_max,  1:global_mean, x - global_mean,   2:global_max, x\n",
    "    other_network_type = 0    \n",
    "    \n",
    "    drop_rate = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialization_parameters_dataset():\n",
    "    global dataset_type, random_data_num, test_data_shitei, learning_data_ratio, validation_data_ratio\n",
    "    global test_data_bigN, bigN, prelearned_model, pickle_filename, file_postfix, model_folder\n",
    "    global adding_split_brain_data, add_split_brain_data_rate, over_sampling, auto_bins, os_fluctuation, bins\n",
    "    global learning_batch_size\n",
    "    \n",
    "    dataset_type = 1       # fix this parameter as 1, do not change it\n",
    "    \n",
    "    # specify random_connection_graph dataset   '1000_567':non_extrapolative setting    '1500':extrapolative setting\n",
    "    random_data_num = '1000_567' \n",
    "    \n",
    "    # specify test dataset [1]:N=5   [2]:N=6   [3]:N=7     0:N=5,6,7 mix (i.e., non_extrapolative setting)\n",
    "    test_data_shitei = 0      # 0 for non_extrapolative setting    [3] for extrapolative setting\n",
    "\n",
    "    # if test_data_shitei is set as 0, all data is shuffled and this proportion of data is utilized in the training process\n",
    "    # if test data shitei is not set as 0, this parameter is ignored \n",
    "    learning_data_ratio = 0.9    # real number [0-1],    0.9 (non-extrapolative setting in the proposed method)\n",
    "\n",
    "    # the proportion of validation dataset within expanded dataset after data augmentation and oversampling \n",
    "    validation_data_ratio = 0.1      # 0.1 in the proposed method      \n",
    "\n",
    "    # execute N=100 graphs as test dataset or not    0:No     2:load saved graphs and execute\n",
    "    # set this parameter 0 for non-extrapolative setting and extrapolative setting\n",
    "    # When setting this parameter as 2, test dataset is replaced with saved graphs of N=100\n",
    "    #           In this case, you should set learning_data_ratio as 1.0\n",
    "    test_data_bigN = 2\n",
    "    \n",
    "    # value of bigN system\n",
    "    bigN = 100       # fix this parameter as 100, do not change it (In the case of test_data_bigN=0, this parameter is ignored.)\n",
    "\n",
    "    # use prelearned models or not    1:Yes,   0:No\n",
    "    # When you set the parameter test_data_bigN as 2, you must set this parameter as 1\n",
    "    # When setting this parameter as 1, this program excecutes only test process and \n",
    "    #    random_data_num can be set as ’1000_567' or '1500', whichever you prefer (the program automatically ignore it).\n",
    "    # When setting this parameter as 1, the program will use models by loading *****_param_*****.pth files from model_folder\n",
    "    prelearned_model = 1\n",
    "\n",
    "    # pkl file name for bigN(=100) graphs\n",
    "    # If you set test_data_bigN as 0, this parameter is ignored.\n",
    "    pickle_filename = 'random_graph_data_N=100/bigN_data_N=100_type9_p=0.0000.pkl' \n",
    "    # Each pickle file includes 100 graphs. First 50 graphs are not split-brain-like systems which are not described in the paper.\n",
    "    # Latter 50 graphs are split-brain-like systems.\n",
    "\n",
    "    # postfix of output files, this parameter is valid only in test_data_bigN=2\n",
    "    file_postfix = '_N=100_type9_p=0.00'   \n",
    "\n",
    "    # folder name where prelearned models exist\n",
    "    model_folder = 'prelearned_model/'\n",
    "\n",
    "    # data augmentation is executed or not  \n",
    "    adding_split_brain_data = 2        # 2:ON    0:OFF\n",
    "    \n",
    "    # the amount of augmented data (if dding_split_brain_data is set as 0, this parameter is ignored)\n",
    "    add_split_brain_data_rate = 0.05   # Specified as a percentage of the number of data used in the training process\n",
    "    \n",
    "    # oversampling is executed or not  \n",
    "    over_sampling = 2          # 2:ON    0:OFF\n",
    "\n",
    "    # bins are automatically determined or not by Kmeans method in oversampling (the number of cluster is fixed as seven in this study)\n",
    "    auto_bins = 1      # 1:ON    0:OFF     We recommend that this parameter should be fixed as 1\n",
    "    \n",
    "    # variables for adding noise in oversampling\n",
    "    os_fluctuation = 2      # 1:big phi and the first feature,  2:big phi and all features (proposed method)\n",
    "    \n",
    "    # bins (if auto_bins is set as 1, this parameter is ignored) \n",
    "    bins = [0.00001, 0.2, 0.4, 0.6, 1.1,  2, 4, np.inf]    \n",
    "   \n",
    "    learning_batch_size = 128     # 128 in this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setting\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric        # package for GNNs\n",
    "from torch_geometric.data import Data, InMemoryDataset       \n",
    "        \n",
    "class BigPhis(InMemoryDataset):       \n",
    "    def __init__(self, data_def, transform = None):   \n",
    "        super(BigPhis, self).__init__('.', transform) \n",
    "        self.data, self.slices = self.collate(data_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Set_path_candidate():\n",
    "    global path_candidate, test_starting_number, random_data_num\n",
    "\n",
    "    if dataset_type == 1 and random_data_num == '1000_567':  # non-extrapolative setting\n",
    "        path_candidate = [\n",
    "            './random_graph_data_non_extrapolative_setting/N=5',\n",
    "            './random_graph_data_non_extrapolative_setting/N=6',\n",
    "            './random_graph_data_non_extrapolative_setting/N=7'\n",
    "            ]\n",
    "    elif dataset_type == 1 and random_data_num == '1500':  # for extrapolative setting\n",
    "        path_candidate = [\n",
    "            './random_graph_data_extrapolative_setting/N=5',\n",
    "            './random_graph_data_extrapolative_setting/N=6',\n",
    "            './random_graph_data_extrapolative_setting/N=7'    \n",
    "            ]\n",
    "    else:\n",
    "        print('Something is wrong')\n",
    "        sys.exit()\n",
    "    \n",
    "\n",
    "    if test_data_shitei != 0:    \n",
    "        specified_indexes = [index - 1 for index in test_data_shitei]\n",
    "\n",
    "        specified_paths = [path_candidate[i] for i in specified_indexes]    \n",
    "        not_specified_paths = [path for i, path in enumerate(path_candidate) if i not in specified_indexes] \n",
    "\n",
    "        path_candidate = not_specified_paths + specified_paths\n",
    "\n",
    "        test_starting_number = len(path_candidate) - len(test_data_shitei) + 1\n",
    "\n",
    "        display('Data path utilized in this experiment', path_candidate)\n",
    "        print('Test data is after the next number (counting the first as 1)', test_starting_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_generation_dataset1():\n",
    "    global max_mc      \n",
    "    max_mc = 0      \n",
    "    \n",
    "    import copy \n",
    "    import re   \n",
    "\n",
    "    pattern_N = r'N=(\\d{1,3})'  \n",
    "    \n",
    "    global data_list, data_count    \n",
    "    \n",
    "    data_count = []  \n",
    "\n",
    "\n",
    "    for path in path_candidate:\n",
    "        match_N = re.search(pattern_N, path)\n",
    "\n",
    "        if match_N:\n",
    "            N = int(match_N.group(1))\n",
    "            print('N:', N)\n",
    "        else:\n",
    "            print('N cannot be found')\n",
    "            sys.exit()\n",
    "\n",
    "            \n",
    "        pattern_path = r'(N=[^\\（]*)'\n",
    "\n",
    "        match = re.search(pattern_path, path)\n",
    "\n",
    "        if match:\n",
    "            path_short = match.group(1).strip('/')\n",
    "            path_short = path_short + '(random)'\n",
    "        else:\n",
    "            print('No match with path')    \n",
    "            sys.exit()\n",
    "\n",
    "            \n",
    "        datafile = pd.read_csv( path + '_meta/summary_N=%d.csv' %(N), index_col=0)                         \n",
    "\n",
    "        total_data_num = len(datafile)\n",
    "        \n",
    "        \n",
    "        for num in range(total_data_num):\n",
    "            T_val = datafile.loc['SN=%d' %(num), 'T']\n",
    "            \n",
    "            out1_list = [ datafile.loc['SN=%d' %(num), out1_items[i] ] for i in range(out1_num) ]\n",
    "\n",
    "            mc_all =  datafile.loc['SN=%d' %(num), 'Cut']\n",
    "            mc_list = mc_all.split('|') \n",
    "            \n",
    "            cm = np.array( pd.read_csv( path + '_meta/connection_SN=%d.csv' %(num), header=None)  ) \n",
    "\n",
    "            src = []\n",
    "            dst = []\n",
    "\n",
    "            for i in range(N):  \n",
    "                for j in range(N): \n",
    "                    if cm[i, j] == 1: \n",
    "                        src.append(i)\n",
    "                        dst.append(j)\n",
    "\n",
    "            J = np.array( pd.read_csv( path + '_meta/edge_val_SN=%d.csv' %(num), header=None)  ) \n",
    "\n",
    "            edge_val = []\n",
    "\n",
    "            for i in range(N):  \n",
    "                for j in range(N): \n",
    "                    if cm[i, j] == 1: \n",
    "                        edge_val.append( [ J[i,j] ] )\n",
    "\n",
    "            state = pd.read_csv( path + '_meta/state_SN=%d.csv' %(num), header=None).iloc[0,:] \n",
    "            state = (state * 2 -1).tolist()\n",
    "                \n",
    "            edge_val_save = copy.deepcopy(edge_val)     \n",
    "            \n",
    "            edge_index = torch.tensor([src, dst], dtype=torch.long) \n",
    "            edge_attr = torch.tensor(edge_val, dtype=torch.float)                    \n",
    "            \n",
    "            if inverse_T == 0:\n",
    "                node_list = [ [x, T_val] for x in state]\n",
    "            elif inverse_T == -1:\n",
    "                node_list = [ [x] for x in state]\n",
    "            else:\n",
    "                print('inverse_T ERROR'); sys.exit()\n",
    "\n",
    "            if feature_degree_on >= 1:      \n",
    "                from collections import Counter\n",
    "\n",
    "                element_counts = Counter(src) \n",
    "                element_counts_list = [element_counts[i] for i in range(N)] \n",
    "\n",
    "                if feature_degree_on == 1:\n",
    "                    node_list = [orig + [add] for orig, add in zip(node_list, element_counts_list)]\n",
    "                else:\n",
    "                    print('feature_degree_on is Wrong!')\n",
    "                    sys.exit()\n",
    "\n",
    "\n",
    "            edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "            data_nx = Data(edge_index=edge_index, num_nodes=N)\n",
    "            G = to_networkx(data_nx,to_undirected=True) \n",
    "\n",
    "            if closeness == 1:\n",
    "                closeness_centrality = nx.closeness_centrality(G)  \n",
    "                closeness_list = [closeness_centrality[node] for node in sorted(G.nodes())]\n",
    "                node_list = [orig + [add] for orig, add in zip(node_list, closeness_list)]                   \n",
    "\n",
    "            if betweenness == 1:\n",
    "                betweenness_centrality = nx.betweenness_centrality(G)\n",
    "                betweenness_list = [betweenness_centrality[node] for node in sorted(G.nodes())]\n",
    "                node_list = [orig + [add] for orig, add in zip(node_list, betweenness_list)]                   \n",
    "\n",
    "            if clustering == 1:\n",
    "                clustering_coefficients = nx.clustering(G)\n",
    "                clustering_list = [clustering_coefficients[node] for node in sorted(G.nodes())]\n",
    "                node_list = [orig + [add] for orig, add in zip(node_list, clustering_list)]    \n",
    "\n",
    "\n",
    "\n",
    "            ################# \n",
    "                        \n",
    "            if feature_prob_on == 2: \n",
    "                inflow = np.zeros(N) \n",
    "\n",
    "                for i in range(len(src)): \n",
    "                    value = edge_val_save[i][0] * node_list[src[i]][0]\n",
    "                    inflow[dst[i]] += value\n",
    "\n",
    "                node_list_first = [row[0] for row in node_list]\n",
    "\n",
    "                prob_val = 1.0 / ( 1.0 + np.exp( -2 * inflow * node_list_first / T_val ) )\n",
    "                prob_val = [pv if pv > 0.5 else 1 - pv for pv in prob_val]\n",
    "                \n",
    "                node_list = [orig + [add] for orig, add in zip(node_list, prob_val)]\n",
    "\n",
    "            \n",
    "            x = torch.tensor(node_list, dtype=torch.float) \n",
    "\n",
    "\n",
    "\n",
    "            #########################################\n",
    "            y = torch.tensor(out1_list, dtype=torch.float)  \n",
    "                    \n",
    "\n",
    "            \n",
    "            multiple_correct_label = []  \n",
    "\n",
    "\n",
    "            if len(mc_list) > max_mc:\n",
    "                max_mc = len(mc_list)\n",
    "\n",
    "            total_mc = 0 \n",
    "\n",
    "            while(total_mc < 2):  \n",
    "\n",
    "                for val in range(len(mc_list)):  \n",
    "                    mc = mc_list[val]  \n",
    "\n",
    "                    mc_former = mc.split('==>')[0] \n",
    "                    mc_latter = mc.split('==>')[1]\n",
    "\n",
    "                    mc_numbers_former_str = re.findall(r'\\d', mc_former)   \n",
    "                    mc_numbers_latter_str = re.findall(r'\\d', mc_latter)\n",
    "\n",
    "                    mc_numbers_former = [int(num) for num in mc_numbers_former_str] \n",
    "                    mc_numbers_latter = [int(num) for num in mc_numbers_latter_str] \n",
    "\n",
    "                    mc_numbers_former.sort()\n",
    "                    mc_numbers_latter.sort()\n",
    "                    \n",
    "                    correct_label = []   \n",
    "\n",
    "                    if class_num == 2: \n",
    "                        for _ in range(N):\n",
    "                            if _ in mc_numbers_former:\n",
    "                                correct_label.append([0,1])\n",
    "                            elif _ in mc_numbers_latter:\n",
    "                                correct_label.append([0,1])\n",
    "                            else:\n",
    "                                correct_label.append([1,0])\n",
    "                    else:\n",
    "                        print('Wrong class_num!!')\n",
    "                        sys.exit()\n",
    "\n",
    "\n",
    "                    multiple_correct_label.append(correct_label)\n",
    "\n",
    "                    total_mc += 1\n",
    "                    if total_mc == 2:\n",
    "                        break\n",
    "\n",
    "\n",
    "                            \n",
    "            y2 = torch.tensor( multiple_correct_label, dtype=torch.float)  \n",
    "            y2 = y2.permute(1,0,2)    \n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, y2=y2, mc_num=len(mc_list), \n",
    "                        meta=[path_short, '-', num, T_val, '-'] )            \n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "        data_count.append( len(data_list) )\n",
    "        \n",
    "    print('Max Major Complex (Degeneracy Degree) is ', max_mc)\n",
    "    print('In our dataset, there is no degeneracy, i.e., the above value must be 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_generation_bigN():\n",
    "    import copy      \n",
    "    import random \n",
    "    \n",
    "    global data_list_bigN   \n",
    "    global pickle_filename\n",
    "    global total_data_num    # the number of saved graphs\n",
    "    \n",
    "    N = bigN  \n",
    "    # total_data_num = 100      # the number of saved graphs\n",
    "\n",
    "\n",
    "    for num in range(total_data_num):\n",
    "        if test_data_bigN == 2:    \n",
    "            if num == 0:\n",
    "                print('loading a graph')\n",
    "                \n",
    "            graph_df = pd.read_pickle(pickle_filename)\n",
    "            T_val = graph_df.loc[num]['T_val']\n",
    "            src = graph_df.loc[num]['src'] \n",
    "            dst = graph_df.loc[num]['dst'] \n",
    "            edge_val = graph_df.loc[num]['edge_val'] \n",
    "            state = graph_df.loc[num]['state'] \n",
    "        else:\n",
    "            print('test_data_bigN is wrong!'); sys.exit()\n",
    "            \n",
    "        edge_val_save = copy.deepcopy(edge_val)    \n",
    "\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long) \n",
    "        edge_attr = torch.tensor(edge_val, dtype=torch.float)                    \n",
    "\n",
    "\n",
    "        if inverse_T == 0:\n",
    "            node_list = [ [x, T_val] for x in state]\n",
    "        elif inverse_T == -1:\n",
    "            node_list = [ [x] for x in state]\n",
    "        else:\n",
    "            print('inverse_T ERROR'); sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "        if feature_degree_on >= 1:      \n",
    "            from collections import Counter\n",
    "\n",
    "            element_counts = Counter(src)        \n",
    "            element_counts_list = [element_counts[i] for i in range(N)]  \n",
    "\n",
    "            if feature_degree_on == 1:\n",
    "                node_list = [orig + [add] for orig, add in zip(node_list, element_counts_list)]\n",
    "            else:\n",
    "                print('feature_degree_on is Wrong!')\n",
    "                sys.exit()\n",
    "\n",
    "\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "        data_nx = Data(edge_index=edge_index, num_nodes=N)\n",
    "        G = to_networkx(data_nx,to_undirected=True) \n",
    "\n",
    "        if closeness == 1:\n",
    "            closeness_centrality = nx.closeness_centrality(G)    \n",
    "            closeness_list = [closeness_centrality[node] for node in sorted(G.nodes())]\n",
    "            node_list = [orig + [add] for orig, add in zip(node_list, closeness_list)]                   \n",
    "\n",
    "        if betweenness == 1:\n",
    "            betweenness_centrality = nx.betweenness_centrality(G)\n",
    "            betweenness_list = [betweenness_centrality[node] for node in sorted(G.nodes())]\n",
    "            node_list = [orig + [add] for orig, add in zip(node_list, betweenness_list)]                   \n",
    "\n",
    "        if clustering == 1:\n",
    "            clustering_coefficients = nx.clustering(G)\n",
    "            clustering_list = [clustering_coefficients[node] for node in sorted(G.nodes())]\n",
    "            node_list = [orig + [add] for orig, add in zip(node_list, clustering_list)]    \n",
    "\n",
    "\n",
    "        if feature_prob_on == 2:   \n",
    "            inflow = np.zeros(N)   \n",
    "\n",
    "            for i in range(len(src)): \n",
    "                value = edge_val_save[i][0] * node_list[src[i]][0]\n",
    "                inflow[dst[i]] += value\n",
    "\n",
    "            node_list_first = [row[0] for row in node_list]\n",
    "\n",
    "            prob_val = 1.0 / ( 1.0 + np.exp( -2 * inflow * node_list_first / T_val ) )\n",
    "            prob_val = [pv if pv > 0.5 else 1 - pv for pv in prob_val]\n",
    "\n",
    "            node_list = [orig + [add] for orig, add in zip(node_list, prob_val)]\n",
    "\n",
    "        x = torch.tensor(node_list, dtype=torch.float)   \n",
    "\n",
    "\n",
    "        ########################################\n",
    "        y = torch.tensor([-100], dtype=torch.float)                         \n",
    "        # As true big phi value is unknown, we set dummy value as -100.\n",
    "        \n",
    "        multiple_correct_label = [[[0,1]] * N]*2     # True labels are also unknown, and we set [0,1] as dummy.\n",
    "        y2 = torch.tensor( multiple_correct_label, dtype=torch.float)  \n",
    "\n",
    "        y2 = y2.permute(1,0,2)    \n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, y2=y2, mc_num=2, \n",
    "                    meta=['big_N', '-', num, T_val, '-'] )            \n",
    "\n",
    "        data_list_bigN.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataset_generation():\n",
    "    global data_list, dataset, feature_dimension, edge_dimension\n",
    "\n",
    "    dataset = BigPhis(data_list)\n",
    "    \n",
    "    print()\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('====================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Number of features: {dataset.num_features -1}') \n",
    "\n",
    "    data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('=============================================================')\n",
    "\n",
    "    # Gather some statistics about the first graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    \n",
    "    feature_dimension = dataset.num_node_features\n",
    "\n",
    "    if feature_s_on == 0:\n",
    "        feature_dimension += -1\n",
    "    \n",
    "    edge_dimension = dataset[0].edge_attr.size(1)\n",
    "    \n",
    "    print('the number of features (Dataset_generation):', feature_dimension)\n",
    "    print('the number of edge features (Dataset_generation):', edge_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataset_generation_bigN():\n",
    "    global data_list_bigN, test_dataset, feature_dimension, edge_dimension\n",
    "\n",
    "    test_dataset = BigPhis(data_list_bigN)\n",
    "\n",
    "    print()\n",
    "    print(f'Dataset: {test_dataset}:')\n",
    "    print('====================')\n",
    "    print(f'Number of graphs: {len(test_dataset)}')\n",
    "    print(f'Number of features: {test_dataset.num_features -1}')  \n",
    "\n",
    "    data = test_dataset[0]  # Get the first graph object.\n",
    "\n",
    "    print()\n",
    "    print(data)\n",
    "    print('==============================================')\n",
    "\n",
    "    # Gather some statistics about the first graph.\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "\n",
    "    feature_dimension = test_dataset.num_node_features  \n",
    "\n",
    "    if feature_s_on == 0:\n",
    "        feature_dimension += -1\n",
    "        \n",
    "    edge_dimension = test_dataset[0].edge_attr.size(1)\n",
    "    \n",
    "    print('the number of features (Dataset_generation):', feature_dimension)\n",
    "    print('the number of edge features (Dataset_generation):', edge_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_split_brain_data(kl_dataset):\n",
    "    import random\n",
    "    import copy      \n",
    "    \n",
    "    degeneracy_num = [4, 2]    \n",
    "    # Our program is made to adapt to degeneracy, but there is no degeneracy in the current dataset. So, do not worry about this parameter.\n",
    " \n",
    "    data_list_local = list(kl_dataset) \n",
    "    \n",
    "    orig_data_num = len(kl_dataset)  \n",
    "\n",
    "    add_data_num = int( orig_data_num * add_split_brain_data_rate )  \n",
    "    \n",
    "    print('the number of original data:', orig_data_num )\n",
    "    print('ratio of added data:', add_split_brain_data_rate, 'and then the number of added data:', add_data_num )\n",
    "        \n",
    "    for _ in range( add_data_num ):\n",
    "        \n",
    "        while(1):\n",
    "            chosen_index1 = random.randint(0, orig_data_num-1 )  \n",
    "            chosen_index2 = random.randint(0, orig_data_num-1 ) \n",
    "            \n",
    "            tmp_data1 = copy.deepcopy( kl_dataset[chosen_index1] )\n",
    "            tmp_data2 = copy.deepcopy( kl_dataset[chosen_index2] )\n",
    "\n",
    "\n",
    "            if torch.abs( tmp_data1.y[0] - tmp_data2.y[0] ) < 0.001: \n",
    "                continue\n",
    "        \n",
    "            if tmp_data1.y[0] < tmp_data2.y[0]:\n",
    "                tmp_data1, tmp_data2 = tmp_data2, tmp_data1 \n",
    "                \n",
    "            break\n",
    "        \n",
    "        num_nodes_data1 = tmp_data1.x.size(0)\n",
    "\n",
    "        shifted_edge_index_data2 = tmp_data2.edge_index + num_nodes_data1\n",
    "\n",
    "        new_edge_index = torch.cat([tmp_data1.edge_index, shifted_edge_index_data2], dim=1)\n",
    "\n",
    "        new_x = torch.cat([tmp_data1.x, tmp_data2.x], dim=0)\n",
    "        new_edge_attr = torch.cat([tmp_data1.edge_attr, tmp_data2.edge_attr], dim=0)\n",
    "\n",
    "        new_y = tmp_data1.y \n",
    "\n",
    "        new_mc_num = tmp_data1.mc_num\n",
    "                \n",
    "        num_nodes_data2 = tmp_data2.x.size(0) \n",
    "        \n",
    "        if class_num == 2:\n",
    "            multiple_correct_label = [[[1,0]] * num_nodes_data2]*degeneracy_num[dataset_type] \n",
    "        \n",
    "        y2 = torch.tensor( multiple_correct_label, dtype=torch.float) \n",
    "        \n",
    "        y2 = y2.permute(1,0,2)    \n",
    "        \n",
    "        new_y2 = torch.cat([tmp_data1.y2, y2], dim=0)\n",
    "\n",
    "        new_meta = []\n",
    "        \n",
    "        for m1, m2 in zip(tmp_data1.meta, tmp_data2.meta):\n",
    "            new_meta.append(f\"{m1}/{m2}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_data = Data(x=new_x, edge_index=new_edge_index, edge_attr=new_edge_attr, y=new_y, y2=new_y2, \n",
    "                        mc_num=new_mc_num, meta=new_meta)\n",
    "         \n",
    "        data_list_local.append(new_data)\n",
    "        \n",
    "    print('the number of training data after augmentation:', len(data_list_local) )\n",
    "    \n",
    "    return data_list_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Over_sampling(kl_dataset):\n",
    "    import random\n",
    "    import copy      \n",
    "    from collections import defaultdict  \n",
    "    \n",
    "    \n",
    "    #####################\n",
    "    global bins\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    if auto_bins == 1:   \n",
    "        target_values = np.array([d.y[0].item() for d in kl_dataset]).reshape(-1, 1)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=7, init='k-means++', n_init=10, random_state=None) \n",
    "        kmeans.fit(target_values)\n",
    "        cluster_centers = np.sort(kmeans.cluster_centers_.flatten()) \n",
    "\n",
    "        bins = np.concatenate(([0.00005], (cluster_centers[:-1] + cluster_centers[1:]) / 2, [np.inf]))  \n",
    "\n",
    "        print('border of bins:', bins)    \n",
    "    else:\n",
    "        print('manual bins:', bins)\n",
    "    ########################\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    categories = defaultdict(list)\n",
    "\n",
    "    for i, d in enumerate(kl_dataset):\n",
    "        category = np.digitize(d.y[0].item(), bins) - 1  \n",
    "        categories[category].append(i)\n",
    "    \n",
    "    max_count = max(len(v) for v in categories.values())\n",
    "    print('the number of samples before oversampling:', [len(v) for v in categories.values()] )\n",
    "    print('the number of samples in the largest bin:', max_count)\n",
    "    \n",
    "    if len(bins) -1 != len( categories.values() ):   \n",
    "        print('Some bins have no members(ERROR)'); sys.exit()\n",
    "    \n",
    "    data_list_local = list(kl_dataset)  \n",
    "    for category, indices in categories.items(): \n",
    "        shortage = max_count - len(indices)\n",
    "        print('Category', category, 'the number of missing samples', shortage)\n",
    "        \n",
    "        if shortage > 0:\n",
    "             for _ in range( int(shortage/1.0) ):\n",
    "                chosen_index = random.choice(indices)\n",
    " \n",
    "                tmp_data =  copy.deepcopy( kl_dataset[chosen_index] )\n",
    "\n",
    " \n",
    "                for val in range(out1_num):\n",
    "                    tmp_data.y[val] *= random.uniform(0.95, 1.05)\n",
    "\n",
    "                if os_fluctuation == 1:\n",
    "                    tmp_data.x[:,1] *= random.uniform(0.95, 1.05)\n",
    "                elif os_fluctuation == 2:\n",
    "                    tmp_data.x[:,:] *= random.uniform(0.95, 1.05)\n",
    "                else:\n",
    "                    print('os_fluctuation is wrong')\n",
    "                    sys.exit()\n",
    "\n",
    "                data_list_local.append(tmp_data)\n",
    "\n",
    "    print('the number of samples after oversampling:', len(data_list_local) )\n",
    "    \n",
    "    return data_list_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Split_dataset():\n",
    "    import copy \n",
    "   \n",
    "    global dataset, train_dataset, validation_dataset, test_dataset, test_starting_number, kougi_learning_dataset\n",
    "\n",
    "    if test_data_shitei == 0:\n",
    "        train_num = int(len(dataset) * learning_data_ratio )      \n",
    "        train2_num = int( train_num * (1-validation_data_ratio) ) \n",
    "\n",
    "        dataset = dataset.shuffle()\n",
    "\n",
    "        train_dataset = dataset[:train2_num]    \n",
    "        validation_dataset = dataset[train2_num:train_num]  \n",
    "        test_dataset = dataset[train_num:]     \n",
    "        \n",
    "        kougi_learning_dataset = dataset[:train_num] \n",
    "        \n",
    "    else:       \n",
    "        kougi_learning_dataset = dataset[: data_count[test_starting_number -2] ]\n",
    "        test_dataset  = dataset[data_count[test_starting_number -2] : ]  \n",
    "        \n",
    "        train2_num = int( len(kougi_learning_dataset) * (1-validation_data_ratio) ) \n",
    "        \n",
    "        \n",
    "        kougi_learning_dataset = kougi_learning_dataset.shuffle()\n",
    "\n",
    "        train_dataset = kougi_learning_dataset[:train2_num] \n",
    "        validation_dataset = kougi_learning_dataset[train2_num:] \n",
    "\n",
    "    \n",
    "    \n",
    "    # data augmentation\n",
    "    if adding_split_brain_data == 2:  \n",
    "        data_list_local = Add_split_brain_data(kougi_learning_dataset) \n",
    "\n",
    "        kougi_learning_dataset = BigPhis(data_list_local)     \n",
    "        kougi_learning_dataset = kougi_learning_dataset.shuffle()\n",
    "        print('the number of training data after augmentation', len(kougi_learning_dataset) )\n",
    "\n",
    "        train2_num = int( len(kougi_learning_dataset) * (1-validation_data_ratio) )  \n",
    "        train_dataset = kougi_learning_dataset[:train2_num] \n",
    "        validation_dataset = kougi_learning_dataset[train2_num:] \n",
    "   \n",
    "\n",
    "    # oversampling\n",
    "    if over_sampling == 2: \n",
    "        data_list_local = Over_sampling(kougi_learning_dataset)   \n",
    "\n",
    "        kougi_learning_dataset = BigPhis(data_list_local)    \n",
    "        kougi_learning_dataset = kougi_learning_dataset.shuffle()\n",
    "        print('the number of training data after oversampling:', len(kougi_learning_dataset) )\n",
    "        \n",
    "        train2_num = int( len(kougi_learning_dataset) * (1-validation_data_ratio) )  \n",
    "        train_dataset = kougi_learning_dataset[:train2_num] \n",
    "        validation_dataset = kougi_learning_dataset[train2_num:]  \n",
    "        \n",
    "    \n",
    "    print(f'Final Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Final Number of validation graphs: {len(validation_dataset)}')\n",
    "    print(f'Final Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input_normalization_new():\n",
    "    global train_dataset, validation_dataset, test_dataset, mean_x, std_x\n",
    "    \n",
    "    if input_normalization == 1:\n",
    "        data_list_local = list(train_dataset)  \n",
    "        \n",
    "        all_features = torch.cat([data.x for data in data_list_local], dim=0)\n",
    "        mean = all_features.mean(dim=0)\n",
    "        std = all_features.std(dim=0)\n",
    "        mean_x = mean \n",
    "        std_x = std\n",
    "\n",
    "        for data in data_list_local:\n",
    "            data.x = (data.x - mean) / std\n",
    "\n",
    "        if feature_s_on == 0:\n",
    "            for data in data_list_local:\n",
    "                data.x = data.x[:, 1:] \n",
    "            \n",
    "        train_dataset = BigPhis(data_list_local)      \n",
    "            \n",
    "            \n",
    "\n",
    "        data_list_local = list(validation_dataset)\n",
    "\n",
    "        for data in data_list_local:\n",
    "            data.x = (data.x - mean) / std\n",
    "\n",
    "        if feature_s_on == 0:\n",
    "            for data in data_list_local:\n",
    "                data.x = data.x[:, 1:]  \n",
    "            \n",
    "        validation_dataset = BigPhis(data_list_local)      \n",
    "\n",
    "      \n",
    "        \n",
    "        #################################\n",
    "        \n",
    "        if prelearned_model == 0: \n",
    "            data_list_local = list(test_dataset)\n",
    "\n",
    "            for data in data_list_local:\n",
    "                data.x = (data.x - mean) / std\n",
    "\n",
    "            if feature_s_on == 0:\n",
    "                for data in data_list_local:\n",
    "                    data.x = data.x[:, 1:] \n",
    "\n",
    "            test_dataset = BigPhis(data_list_local)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1654061669382,
     "user": {
      "displayName": "T Murata",
      "userId": "06657056873172833285"
     },
     "user_tz": -540
    },
    "id": "h5Kg8oiiwWSH",
    "outputId": "3cf1ffeb-471a-4fb3-d37f-7ea838f44013",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Generation_dataloader():\n",
    "    from torch_geometric.loader import DataLoader\n",
    "\n",
    "    global train_dataset, validation_dataset, test_dataset, train_loader, validation_loader, test_loader\n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=learning_batch_size, shuffle=True)    \n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=256, shuffle=False) \n",
    "    \n",
    "    if prelearned_model == 0:    \n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1654062075959,
     "user": {
      "displayName": "T Murata",
      "userId": "06657056873172833285"
     },
     "user_tz": -540
    },
    "id": "vlBGhm_Ux-pe",
    "outputId": "134b5c9e-ca4c-4f26-b259-ae3517f2e668"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear     \n",
    "import torch.nn.functional as F   \n",
    "from torch_geometric.nn import GATConv, GraphConv, TransformerConv \n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn import BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_reg(torch.nn.Module):    \n",
    "    def __init__(self, hidden_channels1, hidden_channels2, hidden_channels3, hidden_channels4, model_type):\n",
    "        \n",
    "        super(GCN_reg, self).__init__()\n",
    "        \n",
    "        if model_type == 'GraphConv':\n",
    "            self.conv1 = GraphConv(feature_dimension, hidden_channels1)   \n",
    "            self.conv2 = GraphConv(hidden_channels1, hidden_channels2)\n",
    "            self.conv3 = GraphConv(hidden_channels2, hidden_channels3)  \n",
    "            self.conv3b = GraphConv(hidden_channels3, hidden_channels4)  \n",
    "        elif model_type == 'GAT':\n",
    "            self.conv1 = GATConv(feature_dimension, hidden_channels1, heads=4, concat=True, edge_dim=edge_dimension)   \n",
    "            self.conv2 = GATConv(hidden_channels1*4, hidden_channels2, heads=4, concat=True, edge_dim=edge_dimension)\n",
    "            self.conv3 = GATConv(hidden_channels2*4, hidden_channels3, heads=4, concat=True, edge_dim=edge_dimension)              \n",
    "            self.conv3b = GATConv(hidden_channels3*4, hidden_channels4, edge_dim=edge_dimension)  \n",
    "        elif model_type == 'Transformer':\n",
    "            self.conv1 = TransformerConv(feature_dimension, hidden_channels1, heads=4, concat=True, edge_dim=edge_dimension)  \n",
    "            self.conv2 = TransformerConv(hidden_channels1*4, hidden_channels2, heads=4, concat=True, edge_dim=edge_dimension)\n",
    "            self.conv3 = TransformerConv(hidden_channels2*4, hidden_channels3, heads=4, concat=True, edge_dim=edge_dimension)               \n",
    "            self.conv3b = TransformerConv(hidden_channels3*4, hidden_channels4, edge_dim=edge_dimension) \n",
    "        else:\n",
    "            print('Something is wrong'); sys.exit()\n",
    "        \n",
    "\n",
    "        \n",
    "        if model_type == 'GAT' or model_type == 'Transformer':\n",
    "            self.batch_norm1 = BatchNorm(hidden_channels1*4)\n",
    "            self.batch_norm2 = BatchNorm(hidden_channels2*4)\n",
    "            self.batch_norm3 = BatchNorm(hidden_channels3*4)\n",
    "        else:     \n",
    "            self.batch_norm1 = BatchNorm(hidden_channels1)\n",
    "            self.batch_norm2 = BatchNorm(hidden_channels2)\n",
    "            self.batch_norm3 = BatchNorm(hidden_channels3)\n",
    "            \n",
    "            \n",
    "        # Branch 1 (Estimating big phi)\n",
    "        self.lin = Linear(hidden_channels4, out1_num)   \n",
    "        \n",
    "        # Branch 2 (Estimating major complex)\n",
    "        self.lin2 = Linear(hidden_channels4, class_num) \n",
    "\n",
    "        # We do not use the following self.att. However, this setting is necessary as a matter of form to load our prelearned models.\n",
    "        self.att = torch.nn.Linear(hidden_channels4*2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        global drop_rate\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        if convolution_type == 'GAT' or convolution_type == 'Transformer':\n",
    "            x0 = self.conv1(x, edge_index, edge_attr=edge_attr) \n",
    "        else:\n",
    "            x0 = self.conv1(x, edge_index) \n",
    "        \n",
    "        x0 = self.batch_norm1(x0)\n",
    "        x0 = x0.relu()\n",
    "        x0 = F.dropout(x0, p=drop_rate, training=self.training)\n",
    "\n",
    "        \n",
    "        if convolution_type == 'GAT' or convolution_type == 'Transformer':\n",
    "            x0 = self.conv2(x0, edge_index, edge_attr=edge_attr) \n",
    "        else:\n",
    "            x0 = self.conv2(x0, edge_index) \n",
    "\n",
    "        x0 = self.batch_norm2(x0)\n",
    "        x0 = x0.relu()\n",
    "        x0 = F.dropout(x0, p=drop_rate, training=self.training)\n",
    "        \n",
    "\n",
    "        if convolution_type == 'GAT' or convolution_type == 'Transformer':\n",
    "            x0 = self.conv3(x0, edge_index, edge_attr=edge_attr) \n",
    "        else:\n",
    "            x0 = self.conv3(x0, edge_index) \n",
    "        \n",
    "        x0 = self.batch_norm3(x0)\n",
    "        x0 = x0.relu()\n",
    "        x0 = F.dropout(x0, p=drop_rate, training=self.training)\n",
    "\n",
    "        \n",
    "        if convolution_type == 'GAT' or convolution_type == 'Transformer':\n",
    "            x0 = self.conv3b(x0, edge_index, edge_attr=edge_attr) \n",
    "        else:\n",
    "            x0 = self.conv3b(x0, edge_index) \n",
    "\n",
    "        \n",
    "           \n",
    "\n",
    "        # 2. Readout layer\n",
    "        if other_network_type == 0 or other_network_type == 2:\n",
    "            x1 = global_max_pool(x0, batch)  \n",
    "        elif other_network_type == 1:\n",
    "            x1 = global_mean_pool(x0, batch) \n",
    "        else:\n",
    "            print('other_network_type is wrong!!')\n",
    "            sys.exit()\n",
    "            \n",
    "        \n",
    "        \n",
    "        # 3. Apply a final classifier\n",
    "        x2 = F.dropout(x1, p=drop_rate, training=self.training)\n",
    "        x2 = self.lin(x2)\n",
    "\n",
    "        ####### \n",
    "        if other_network_type == 0 or other_network_type == 1:\n",
    "            x3 = x0 - x1[batch]    \n",
    "        elif other_network_type == 2:\n",
    "            x3 = x0\n",
    "        else:\n",
    "            print('other_network_type is wrong!!')\n",
    "            sys.exit()\n",
    "            \n",
    "        x3 = F.dropout(x3, p=drop_rate, training=self.training) \n",
    "        x3 = self.lin2(x3)\n",
    "        \n",
    "         \n",
    "        return x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_MSE(prediction, target):    \n",
    "    squared_errors = (prediction - target) ** 2\n",
    "    weighted_squared_errors = squared_errors * out1_weight_tensor\n",
    "    loss = weighted_squared_errors.mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train():\n",
    "    model.train()\n",
    "\n",
    "    all_loss = 0  \n",
    "    all_loss1 = 0\n",
    "    all_loss2 = 0\n",
    "  \n",
    "    \n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        data = data.to(device)    \n",
    "        out1, out2 = model(data.x, data.edge_index, data.edge_attr, data.batch)  # Perform a single forward pass.\n",
    "        \n",
    "        data.y = data.y.float()    \n",
    "        data.y = data.y.view(-1, out1_num) \n",
    "\n",
    "        data.y2 = data.y2.float()   \n",
    "        \n",
    "        loss1 = Weighted_MSE(out1, data.y)  # Compute the loss. \n",
    "        loss2 = criterion2(out2, data.y2[:, 0, :]) # Compute the loss.\n",
    "        \n",
    "        sum_loss = loss1 + loss2_coef*loss2\n",
    "        sum_loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        \n",
    "        all_loss += (loss1 + loss2_coef*loss2) * len(data)\n",
    "        all_loss1 += loss1 * len(data)\n",
    "        all_loss2 += loss2_coef * loss2 * len(data)\n",
    "        \n",
    "\n",
    "    return all_loss / len(train_loader.dataset), all_loss1 / len(train_loader.dataset), all_loss2 / len(train_loader.dataset)\n",
    "\n",
    "        \n",
    "def Validation():\n",
    "    model.eval()    \n",
    "\n",
    "    all_loss = 0  \n",
    "    all_loss1 = 0\n",
    "    all_loss2 = 0\n",
    "    \n",
    "    \n",
    "    for data in validation_loader:  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)   \n",
    "        out1, out2 = model(data.x, data.edge_index, data.edge_attr, data.batch)  # Perform a single forward pass.\n",
    "\n",
    "        data.y = data.y.float()   \n",
    "        data.y = data.y.view(-1, out1_num) \n",
    "\n",
    "        data.y2 = data.y2.float()   \n",
    "\n",
    "        loss1 = Weighted_MSE(out1, data.y)  # Compute the loss.  \n",
    "        loss2 = criterion2(out2, data.y2[:, 0, :]) # Compute the loss.  \n",
    "\n",
    "        all_loss += (loss1 + loss2_coef*loss2) * len(data)\n",
    "        all_loss1 += loss1 * len(data)\n",
    "        all_loss2 += loss2_coef * loss2 * len(data)\n",
    "\n",
    "    return all_loss / len(validation_loader.dataset), all_loss1 / len(validation_loader.dataset), all_loss2 / len(validation_loader.dataset)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def Test(loader):   \n",
    "    model.eval()\n",
    "\n",
    "    error1 = 0    \n",
    "    error2 = 0    \n",
    "    accuracy = 0   \n",
    "    exact_match_accuracy = 0      \n",
    "    total_node_count = 0\n",
    "\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        out1, out2 = model(data.x, data.edge_index, data.edge_attr,  data.batch)  \n",
    "        pred = out1   \n",
    "\n",
    "        data.y = data.y.float()  \n",
    "        data.y = data.y.view(-1, out1_num)  \n",
    "\n",
    "        data.y2 = data.y2.float()   \n",
    "        error1 += ((pred - data.y)*(pred - data.y)).sum(dim=0)  # Check against ground-truth labels.\n",
    "        error2 += torch.abs( pred - data.y ).sum(dim=0)\n",
    "        \n",
    "        true_labels = torch.argmax(data.y2[:, 0, :], dim=1) \n",
    "        predicted_labels = torch.argmax(out2, dim=1) \n",
    "        accuracy += torch.sum(predicted_labels == true_labels).item()   \n",
    "        exact_match_accuracy += torch.sum(torch.all(predicted_labels == true_labels)).item()  \n",
    "       \n",
    "        \n",
    "        total_node_count += data.num_nodes\n",
    "        \n",
    "    \n",
    "    return error1 / len(loader.dataset), error2/ len(loader.dataset), \\\n",
    "            exact_match_accuracy / len(loader.dataset), accuracy / total_node_count  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Detail_record(loader, output_flag, prefix, time_str):    \n",
    "    model.eval()\n",
    "\n",
    "    degeneracy_count = 0  \n",
    "    degeneracy_exact_count = 0  \n",
    "    \n",
    "    # For final results \n",
    "    correct_big_phi_list = []\n",
    "    predicted_big_phi_list = []\n",
    "    mse_list = []\n",
    "    mae_list = []\n",
    "    maer_list = []   \n",
    "    true_labels_list = []\n",
    "    predicted_labels_list = []\n",
    "    node_num_list = []\n",
    "    bit_rate_list = []\n",
    "    \n",
    "    # For meta info.\n",
    "    path_list = []\n",
    "    fr_list = []\n",
    "    state_list = []\n",
    "    T_list = []\n",
    "    reverse_list = []\n",
    "\n",
    "    \n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out1, out2 = model(data.x, data.edge_index, data.edge_attr, data.batch)  \n",
    "        \n",
    "        pred = out1    \n",
    "\n",
    "        data.y = data.y.float()   \n",
    "        data.y = data.y.view(-1, out1_num)  \n",
    "\n",
    "        error1 = ((pred - data.y)*(pred - data.y))  # Check against ground-truth labels.\n",
    "        error2 = torch.abs( pred - data.y )\n",
    "\n",
    "        maer = torch.abs((pred - data.y) / (data.y + 1e-10))\n",
    "\n",
    "        correct_big_phi_list += data.y.tolist() \n",
    "        predicted_big_phi_list += pred.tolist() \n",
    "        mse_list += error1.tolist()\n",
    "        mae_list += error2.tolist()\n",
    "        maer_list += maer.tolist()    \n",
    "        \n",
    "        data.y = data.y.view(-1)  \n",
    "        \n",
    "        \n",
    "        ############ Major Complex\n",
    "        data_list_local = data.to_data_list()      \n",
    "        node_num = [ data_list_local[num].x.shape[0] for num in range(len(data_list_local)) ]    \n",
    "        node_num_list += node_num\n",
    "\n",
    "        \n",
    "        data.y2 = data.y2.float()   \n",
    "\n",
    "        predicted_labels = torch.argmax(out2, dim=1)  \n",
    "        node_start_index = 0\n",
    "\n",
    "        for batch_index, num_nodes in enumerate(node_num):  \n",
    "            max_accuracy = -10.0\n",
    "            best_true_labels = None  \n",
    "            graph_predicted_labels = predicted_labels[ node_start_index: node_start_index+num_nodes]  \n",
    "            predicted_labels_list.append(graph_predicted_labels.tolist())  \n",
    "            \n",
    "            for i in range(data.y2.size(1)): \n",
    "                true_labels = torch.argmax(data.y2[:, i, :], dim=1)[node_start_index:node_start_index+num_nodes]\n",
    "\n",
    "                correct_predictions = (graph_predicted_labels == true_labels).float().mean()\n",
    "\n",
    "                if correct_predictions > max_accuracy:\n",
    "                    if i >= 1:\n",
    "                        degeneracy_count += 1\n",
    "                        if correct_predictions > 0.9999:\n",
    "                            degeneracy_exact_count += 1\n",
    "                        \n",
    "                    max_accuracy = correct_predictions\n",
    "                    best_true_labels = true_labels  \n",
    "\n",
    "            bit_rate_list.append(max_accuracy.item())\n",
    "            true_labels_list.append(best_true_labels.tolist())  \n",
    "            node_start_index += num_nodes \n",
    "\n",
    "\n",
    "            \n",
    "        ############ \n",
    "        path_local = [ data_list_local[num].meta[0] for num in range(len(data_list_local)) ]    \n",
    "        path_list += path_local\n",
    "        \n",
    "        fr_local = [ data_list_local[num].meta[1] for num in range(len(data_list_local)) ]  \n",
    "        fr_list += fr_local\n",
    "        \n",
    "        state_local = [ data_list_local[num].meta[2] for num in range(len(data_list_local)) ]   \n",
    "        state_list += state_local\n",
    "        \n",
    "        T_local = [ data_list_local[num].meta[3] for num in range(len(data_list_local)) ]    \n",
    "        T_list += T_local\n",
    "        \n",
    "        reverse_local = [ data_list_local[num].meta[4] for num in range(len(data_list_local)) ]    \n",
    "        reverse_list += reverse_local\n",
    "        \n",
    "        \n",
    " \n",
    "    ###### \n",
    "    \n",
    "    data_dict = {}\n",
    "\n",
    "    for name, pred_col, corr_col, mse_col, mae_col, maer_col in zip(out1_index, zip(*predicted_big_phi_list), zip(*correct_big_phi_list), zip(*mse_list), zip(*mae_list), zip(*maer_list)):\n",
    "        data_dict[f'Estimate{name}'] = pred_col\n",
    "        data_dict[f'True{name}'] = corr_col\n",
    "        data_dict[f'MSE{name}']  = mse_col\n",
    "        data_dict[f'MAE{name}']  = mae_col\n",
    "        data_dict[f'MAER{name}'] = maer_col  \n",
    "\n",
    "\n",
    "    data_dict.update({\n",
    "        'True_label': true_labels_list, \n",
    "        'Est_label': predicted_labels_list, \n",
    "        'Node_num': node_num_list, \n",
    "        'Bit_rate': bit_rate_list\n",
    "        })       \n",
    "        \n",
    "    result_df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    result_df['Exact_match'] = (result_df['True_label'] == result_df['Est_label']).astype(int)\n",
    "    \n",
    "    result_df['True_label(replaced)'] = result_df['True_label'].apply(replace_2_with_1)\n",
    "    result_df['Est_label(replaced)'] = result_df['Est_label'].apply(replace_2_with_1)\n",
    "        \n",
    "    result_df['Bit_rate(replaced)'] = result_df.apply( \\\n",
    "        lambda row: calculate_bit_accuracy(row['True_label(replaced)'], row['Est_label(replaced)']), axis=1)\n",
    "\n",
    "    result_df['Exact_match(replaced)'] = (result_df['True_label(replaced)'] == result_df['Est_label(replaced)']).astype(int)\n",
    "    \n",
    "    \n",
    "    result_df['Topology'] = path_list\n",
    "    \n",
    "    if dataset_type == 0:\n",
    "        result_df['fr'] = fr_list\n",
    "        \n",
    "    if dataset_type == 0:    \n",
    "        result_df['state_num'] = state_list\n",
    "    else:\n",
    "        result_df['data_num'] = state_list\n",
    "        \n",
    "        \n",
    "    result_df['T'] = T_list\n",
    "    result_df['Reverse'] = reverse_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###### Output for display\n",
    "    average_rate_normal = (result_df['Node_num']*result_df['Bit_rate']).sum() / result_df['Node_num'].sum()\n",
    "    average_rate_replace = (result_df['Node_num']*result_df['Bit_rate(replaced)']).sum() / result_df['Node_num'].sum()\n",
    "    \n",
    "    print('Bit_rate(Normal/Replaced):', average_rate_normal, '/', average_rate_replace )    \n",
    "    print('Exact_match_rate(Normal/Replaced):', result_df['Exact_match'].mean(), '/', result_df['Exact_match(replaced)'].mean() )\n",
    "\n",
    "    \n",
    "    print('Degeneracy Count', degeneracy_count, 'Degeneracy Exact_match Count', degeneracy_exact_count)\n",
    "    \n",
    "    ###### File output\n",
    "    if output_flag == 1:\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        \n",
    "        file_name = f'{save_folder+\"final_\"+prefix+\"_\"+str(degeneracy_count)+\"_\"+str(degeneracy_exact_count)+\"_\"+\"result_\"+time_str+\".xlsx\"}'\n",
    "        result_df.to_excel(file_name)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "###################### Function definition ##################################\n",
    "def replace_2_with_1(lst):\n",
    "    return [1 if x == 2 else x for x in lst]\n",
    "\n",
    "def calculate_bit_accuracy(true_list, predicted_list):\n",
    "    correct = sum(a == b for a, b in zip(true_list, predicted_list))\n",
    "    total = len(true_list)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prelearned_model_utilization():\n",
    "    import glob\n",
    "    import re\n",
    "    import copy  \n",
    "\n",
    "    global initial_output_flag, model, test_dataset, file_paths\n",
    "\n",
    "    \n",
    "    test_dataset_raw = copy.deepcopy(test_dataset)     \n",
    "\n",
    "    file_paths = glob.glob(os.path.join(model_folder, 'model_param_*.pth'))\n",
    "\n",
    "\n",
    "    for file_path in file_paths:        \n",
    "        test_dataset = test_dataset_raw \n",
    "\n",
    "        pattern = r\"\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\"\n",
    "\n",
    "        train_time = re.search(pattern, file_path)\n",
    "\n",
    "        if train_time:\n",
    "            print(\"Extracted datetime:\", train_time.group())\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "\n",
    "        model.load_state_dict(torch.load( file_path ) )\n",
    "\n",
    "        model = model.to(device) \n",
    "        \n",
    "        \n",
    "        tmp_data = pd.read_csv(model_folder + 'train_mean_std_' + train_time.group() + '.csv', header=None)\n",
    "\n",
    "        mean = tmp_data[0].values\n",
    "        std = tmp_data[1].values\n",
    "        print('Mean and Std in the learnig process', mean[1:], std[1:])\n",
    "        \n",
    "        data_list_local = list(test_dataset)\n",
    "\n",
    "        for data in data_list_local:\n",
    "            data.x = (data.x - mean) / std\n",
    "            data.x = data.x.float()  \n",
    "\n",
    "        if feature_s_on == 0:\n",
    "            for data in data_list_local:\n",
    "                data.x = data.x[:, 1:] \n",
    "\n",
    "        test_dataset = BigPhis(data_list_local)    \n",
    "        \n",
    "        from torch_geometric.loader import DataLoader\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)     \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        ########### Detailed results output\n",
    "        current_time = datetime.datetime.now()\n",
    "        time_str = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        print('\\nTest dataset')\n",
    "        result_test_df = Detail_record(test_loader, 1, 'test', train_time.group() + '_TO_' + time_str + file_postfix)\n",
    "        \n",
    "\n",
    "        ######## Summary of results output\n",
    "        msel = []      # l:learning\n",
    "        mael = []\n",
    "        maerl = [] \n",
    "        corl = []\n",
    "\n",
    "        for val in range(out1_num):\n",
    "            msel.append( -9999 )    # As learning process is not executed, we set -9999 as dummy\n",
    "            mael.append( -9999 )\n",
    "            maerl.append( -9999 )\n",
    "            corl.append( -9999 )\n",
    "\n",
    "        ba1l = -9999\n",
    "        em1l = -9999\n",
    "        ba2l = -9999\n",
    "        em2l = -9999\n",
    "\n",
    "        mset = []     # t:test\n",
    "        maet = []\n",
    "        maert = [] \n",
    "        cort = []\n",
    "\n",
    "        for val in range(out1_num):\n",
    "            mset.append( result_test_df[ 'MSE'+out1_index[val] ].mean() )\n",
    "            maet.append( result_test_df[ 'MAE'+out1_index[val] ].mean() )\n",
    "            maert.append( result_test_df[ 'MAER'+out1_index[val] ].mean() ) \n",
    "            cort.append( result_test_df[ ['Estimate'+out1_index[val], 'True'+out1_index[val] ] ].corr().iloc[0, 1] )\n",
    "\n",
    "        ba1t = result_test_df['Bit_rate'].mean()\n",
    "        em1t = result_test_df['Exact_match'].mean()\n",
    "        ba2t = result_test_df['Bit_rate(replaced)'].mean()\n",
    "        em2t = result_test_df['Exact_match(replaced)'].mean()\n",
    "\n",
    "        mseL_label = []\n",
    "        maeL_label = []\n",
    "        maerL_label = [] \n",
    "        corL_label = []\n",
    "        mseT_label = []\n",
    "        maeT_label = []\n",
    "        maerT_label = []  \n",
    "        corT_label = []\n",
    "\n",
    "        for val in range(out1_num):\n",
    "            mseL_label.append( 'mseL_'+out1_index[val] )\n",
    "            maeL_label.append( 'maeL_'+out1_index[val] )\n",
    "            maerL_label.append( 'maerL_'+out1_index[val] )\n",
    "            corL_label.append( 'corrL_'+out1_index[val] )\n",
    "            mseT_label.append( 'mseT_'+out1_index[val] )\n",
    "            maeT_label.append( 'maeT_'+out1_index[val] )\n",
    "            maerT_label.append( 'maerT_'+out1_index[val] )\n",
    "            corT_label.append( 'corrT_'+out1_index[val] )            \n",
    "\n",
    "            \n",
    "\n",
    "        epoch = -9999\n",
    "        val_loss = torch.tensor(-9999)\n",
    "        data_list = []    \n",
    "        train_dataset = []\n",
    "        validation_dataset = []\n",
    "\n",
    "        \n",
    "        ###############################\n",
    "        #\n",
    "        file_name = save_folder + summary_result_filename + '.csv' \n",
    "        headers = ['time', 'epoch', *mseL_label, *maeL_label, *maerL_label, *corL_label, \n",
    "                   'Bit_rateRowL', 'Exact_matchRowL', 'Bit_rateRepL', 'Exact_matchRepL', \n",
    "                   *mseT_label, *maeT_label, *maerT_label, *corT_label, \n",
    "                   'Bit_rateRowT', 'Exact_matchRowT', 'Bit_rateRepT', 'Exact_matchRepT', 'val_loss', \n",
    "                   'dataset_type', 'f_s_on',\n",
    "                   'f_deg_on', 'f_prob_on', 'class_num', 'input_norm', 'inverse_T', \n",
    "                   'closeness', 'betweenness', 'clustering', 'optimizer', 'lr', 'wei_decay', \n",
    "                   'out1_num', 'out1_weight', 'class_weight', 'patience', 'loss2_coef', 'batch', \n",
    "                   'test_shitei', 'random_num', 'learning_data_ratio', 'val_data_ratio', \n",
    "                   'augmentation', 'aug_rate', 'over_sample', 'auto_bin', 'os_bin', 'os_fluc', \n",
    "                   'time', 'convolution', 'other', 'model', 'drop_p', 'orig_data_num', 'trainset_num', 'valset_num', 'testset_num']\n",
    "\n",
    "        data_to_save = [time_str, epoch, *msel, *mael, *maerl, *corl, ba1l, em1l, ba2l, em2l, \n",
    "                        *mset, *maet, *maert, *cort, ba1t, em1t, ba2t, em2t, val_loss.item(), \n",
    "                        dataset_type, feature_s_on, feature_degree_on, feature_prob_on, \n",
    "                        class_num, input_normalization, inverse_T, closeness, betweenness,\n",
    "                        clustering, \n",
    "                        optimizer_select, learning_rate, weight_decay_val, out1_num, str(out1_weight), \n",
    "                        str(weights), patience_val, loss2_coef, \n",
    "                        learning_batch_size,\n",
    "                        str(test_data_shitei), str(random_data_num), learning_data_ratio, validation_data_ratio, \n",
    "                        adding_split_brain_data, add_split_brain_data_rate, over_sampling, auto_bins,\n",
    "                        str(np.round(bins, 3)), os_fluctuation,\n",
    "                        time_str, convolution_type, other_network_type, str(model).replace('\\n', '; '), drop_rate,\n",
    "                        len(data_list), len(train_dataset), len(validation_dataset), len(test_dataset)]\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(file_name):\n",
    "                with open(file_name, 'w', newline='', encoding='shift-jis') as f:  \n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(headers)\n",
    "                    writer.writerow(data_to_save)\n",
    "                    initial_output_flag = 0\n",
    "            else:\n",
    "                with open(file_name, 'a', newline='', encoding='shift-jis') as f:\n",
    "                    writer = csv.writer(f)\n",
    "\n",
    "                    if initial_output_flag == 1:  \n",
    "                        writer.writerow(headers)\n",
    "                        initial_output_flag = 0\n",
    "\n",
    "                    writer.writerow(data_to_save)\n",
    "\n",
    "        except IOError:\n",
    "            new_file_name = f'{save_folder+\"TMP_\"+summary_result_filename+\"_\"+time_str+\".csv\"}'\n",
    "            with open(new_file_name, 'w', newline='', encoding='shift-jis') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(headers)\n",
    "                writer.writerow(data_to_save)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################### Main Cell\n",
    "# the number of output channels\n",
    "hc1 = 50        \n",
    "hc2 = 150 \n",
    "hc3 = 150 \n",
    "hc4 = 50 \n",
    "\n",
    "# Training error and test error are displayed each epoch or not    1:OFF (time-saving; recommended)    0:ON(time-consuming)\n",
    "time_save = 1   \n",
    "\n",
    "# folder name where output files are generated \n",
    "save_folder = 'result/'      \n",
    "\n",
    "# name for summary of results (CSV files are output) \n",
    "summary_result_filename = 'result_summary' \n",
    "\n",
    "\n",
    "# flag utilized internally, do not change it \n",
    "initial_output_flag = 1 \n",
    "\n",
    "\n",
    "\n",
    "global save_folder, summary_result_filename, initial_output_flag, model, pickle_filename, file_postfix\n",
    "global feature_degree_on, feature_prob_on, class_num, input_normalization\n",
    "global inverse_T, closeness, betweenness\n",
    "global learning_rate, weight_decay_val, out1_weight_tensor, weights, patience_val, loss2_coef, optimizer_select\n",
    "global convolution_type, other_network_type\n",
    "global test_data_shitei, learning_data_ratio, validation_data_ratio, over_sampling, auto_bins, bins, os_fluctuation\n",
    "global learning_batch_size\n",
    "global random_data_num\n",
    "global total_data_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Initialization_parameters_node_edge()\n",
    "Initialization_parameters_optimization()\n",
    "Initialization_parameters_GNN()\n",
    "Initialization_parameters_dataset()\n",
    "                \n",
    "            \n",
    "##############################################################################\n",
    "#### Uncomment the one section you want to run and comment out the other. ####\n",
    "##############################################################################\n",
    "\n",
    "### [1] For comparison experiments (the first and the second experiments in the paper)  \n",
    "'''  \n",
    "test_data_bigN = 0; prelearned_model = 0\n",
    "\n",
    "# specify random_connection_graph dataset   \n",
    "# CHOOSE ONE  ==>  '1000_567':non_extrapolative setting  or '1500':extrapolative setting\n",
    "#### random_data_num = '1000_567'; test_data_shitei = 0 \n",
    "#### random_data_num = '1500'; test_data_shitei = [3] \n",
    "\n",
    "for inverse_T, feature_prob_on, feature_degree_on, closeness, betweenness, clustering, loss2_coef, convolution_type, optimizer_select, other_network_type in [\n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'Lion', 0),\n",
    "            (-1, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'Lion', 0), \n",
    "            (0, 0, 1, 1, 1, 1, 5.0, 'Transformer', 'Lion', 0),     \n",
    "            (0, 2, 0, 1, 1, 1, 5.0, 'Transformer', 'Lion', 0),\n",
    "            (0, 2, 1, 0, 1, 1, 5.0, 'Transformer', 'Lion', 0), \n",
    "            (0, 2, 1, 1, 0, 1, 5.0, 'Transformer', 'Lion', 0),\n",
    "            (0, 2, 1, 1, 1, 0, 5.0, 'Transformer', 'Lion', 0),\n",
    "            (0, 2, 1, 1, 1, 1, 0.0, 'Transformer', 'Lion', 0),\n",
    "            (0, 2, 1, 1, 1, 1, 100000.0, 'Transformer', 'Lion', 0),\n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'GAT', 'Lion', 0), \n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'GraphConv', 'Lion', 0), \n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'Adam', 0),\n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'RAdam', 0),\n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'AdamW', 0), \n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'Lion', 1),\n",
    "            (0, 2, 1, 1, 1, 1, 5.0, 'Transformer', 'Lion', 2) \n",
    "            ]:\n",
    "    for iteration in range(100):\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "### [2] For experiments with N=100 systems\n",
    "'''\n",
    "test_data_bigN = 2; prelearned_model = 1; total_data_num = 100 \n",
    "\n",
    "for p_e in [0.0000, 0.0004, 0.002, 0.004, 0.006, 0.008, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "                                pickle_filename = 'random_graph_data_N=100/bigN_data_N=100_type9_p=%.4lf.pkl' %(p_e)\n",
    "                                file_postfix = '_N=100_type9_p=%.4lf' %(p_e) \n",
    "                                \n",
    "                                print(pickle_filename)\n",
    "'''                            \n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "### [3] For experiments on scaling behavior with three topologies (N=10-100)\n",
    "'''\n",
    "test_data_bigN = 2; prelearned_model = 1; total_data_num = 30 \n",
    "\n",
    "for N in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "                                pickle_filename = 'three_topologies_data_N=10-100/bigN_data_N=%d.pkl' %(N)\n",
    "                                file_postfix = '_N=%d' %(N) \n",
    "\n",
    "                                bigN = N\n",
    "        \n",
    "                                print(pickle_filename)\n",
    "'''\n",
    "\n",
    "\n",
    "                                clear_output(True) \n",
    "                                \n",
    "                                Set_path_candidate()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "                                if prelearned_model == 0 or (prelearned_model==1 and test_data_bigN == 0):\n",
    "                                    data_list = []\n",
    "\n",
    "                                    if dataset_type == 1:\n",
    "                                        Data_generation_dataset1()\n",
    "                                    else:\n",
    "                                        print('dataset_type is wrong!'); sys.exit()\n",
    "                                        \n",
    "                                    Dataset_generation()\n",
    "                                    Split_dataset()\n",
    "                                \n",
    "                                if test_data_bigN == 2:\n",
    "                                    data_list_bigN = []\n",
    "                                    Data_generation_bigN()\n",
    "                                    Dataset_generation_bigN()\n",
    "                                \n",
    "                                if prelearned_model == 0 or (prelearned_model==1 and test_data_bigN == 0):\n",
    "                                    Input_normalization_new()\n",
    "                                    Generation_dataloader()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                ######################################################################\n",
    "                                ######################################################################\n",
    "                                model = GCN_reg(hidden_channels1=hc1, hidden_channels2=hc2, hidden_channels3=hc3, hidden_channels4=hc4, model_type=convolution_type)  \n",
    "                                \n",
    "                                if optimizer_select == 'Adam':\n",
    "                                    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_val)\n",
    "                                elif optimizer_select == 'Lion':    \n",
    "                                    optimizer = Lion(model.parameters(), lr=learning_rate, weight_decay=weight_decay_val)\n",
    "                                elif optimizer_select == 'RAdam':\n",
    "                                    optimizer = torch.optim.RAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_val)\n",
    "                                elif optimizer_select == 'AdamW':\n",
    "                                    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay_val)\n",
    "                                else:\n",
    "                                    print('Optimizer select is wrong!'); sys.exit()\n",
    "\n",
    "\n",
    "                                out1_weight_tensor = torch.tensor( out1_weight )\n",
    "                                out1_weight_tensor = out1_weight_tensor.to(device)   \n",
    "                                \n",
    "                                class_weights = torch.FloatTensor(weights).cuda()\n",
    "                                criterion2 = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "                                early_stopping = EarlyStopping(patience=patience_val)   \n",
    "                                \n",
    "                                if prelearned_model == 1:  \n",
    "                                    Prelearned_model_utilization()     \n",
    "                                    continue\n",
    "                                \n",
    "                                model = model.to(device)   \n",
    "\n",
    "        \n",
    "                                ##########################################################\n",
    "                                ##################### Epoch loop ########################\n",
    "                                #########################################################\n",
    "                                loss_history = []\n",
    "\n",
    "                                for epoch in range(1, 5000):\n",
    "                                    train_loss, train_loss1, train_loss2 = Train()\n",
    "                                    \n",
    "                                    \n",
    "                                    if time_save == 1:   \n",
    "                                        train_error1, train_error2, train_exact_match, train_bit_accuracy = [-10], [-10], -10, -10\n",
    "                                        test_error1, test_error2, test_exact_match, test_bit_accuracy = [-10], [-10], -10, -10\n",
    "                                    else:\n",
    "                                        train_error1, train_error2, train_exact_match, train_bit_accuracy = Test(train_loader)\n",
    "                                        test_error1, test_error2, test_exact_match, test_bit_accuracy = Test(test_loader)\n",
    "                                    \n",
    "                                    val_loss, val_loss1, val_loss2 = Validation()\n",
    "                                    \n",
    "                                    train_error1_print = \", \".join(f\"{error:.3f}\" for error in train_error1)\n",
    "                                    train_error2_print = \", \".join(f\"{error:.3f}\" for error in train_error2)\n",
    "                                    test_error1_print = \", \".join(f\"{error:.3f}\" for error in test_error1)\n",
    "                                    test_error2_print = \", \".join(f\"{error:.3f}\" for error in test_error2)\n",
    "                                    \n",
    "                                    print(f'Epoch: {epoch:03d}, [Train] mse: {train_error1_print}, mae: {train_error2_print}, em: {train_exact_match:.3f}, ba: {train_bit_accuracy:.3f}' )\n",
    "                                    print(f'[Test] MSE: {test_error1_print}, MAE: {test_error2_print}, EM: {test_exact_match:.3f}, BA: {test_bit_accuracy:.3f}')\n",
    "                                    if time_save == 0:\n",
    "                                        print('NOTE: Before the early stop, exact-match (em) is correctly obtained only if mini-batch size is set as 1.')\n",
    "                                    print(f'[Val] loss: {val_loss} \\n')\n",
    "\n",
    "                                    loss_history.append( \n",
    "                                        [train_loss.item(), train_loss1.item(), train_loss2.item(), val_loss.item(), val_loss1.item(), val_loss2.item()] )\n",
    "\n",
    "                                    early_stopping(val_loss, model)\n",
    "\n",
    "                                    if early_stopping.early_stop:\n",
    "                                        print('Early Stop!!')\n",
    "                                        break\n",
    "\n",
    "\n",
    "                                ######## After early stopping, load the optimal model\n",
    "                                model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                                \n",
    "                                train_error1, train_error2, train_exact_match, train_bit_accuracy = Test(train_loader)\n",
    "                                test_error1, test_error2, test_exact_match, test_bit_accuracy = Test(test_loader)\n",
    "                                val_loss, val_loss1, val_loss2 = Validation()\n",
    "                                \n",
    "                                train_error1_print = \", \".join(f\"{error:.3f}\" for error in train_error1)\n",
    "                                train_error2_print = \", \".join(f\"{error:.3f}\" for error in train_error2)\n",
    "                                test_error1_print = \", \".join(f\"{error:.3f}\" for error in test_error1)\n",
    "                                test_error2_print = \", \".join(f\"{error:.3f}\" for error in test_error2)                                \n",
    "                                \n",
    "                                print('\\n')\n",
    "                                print(f'Optimal, [Train] mse: {train_error1_print}, mae: {train_error2_print}, em: {train_exact_match:.3f}, ba: {train_bit_accuracy:.3f}') \n",
    "                                print(f'Optimal, [Test] MSE: {test_error1_print}, MAE: {test_error2_print}, EM: {test_exact_match:.3f}, BA: {test_bit_accuracy:.3f}')\n",
    "                                print(f'[Val] loss: {val_loss} \\n')\n",
    "\n",
    "                                \n",
    "                                ########### Detailed results output\n",
    "\n",
    "                                if not os.path.exists(save_folder):\n",
    "                                    os.makedirs(save_folder)\n",
    "\n",
    "                                current_time = datetime.datetime.now()\n",
    "\n",
    "                                time_str = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "                                print('Learning Dataset')\n",
    "                                result_learning_df = Detail_record(train_loader, 1, 'train', time_str)\n",
    "\n",
    "                                print('\\nTest Dataset')\n",
    "                                result_test_df = Detail_record(test_loader, 1, 'test', time_str)\n",
    "\n",
    "                                \n",
    "                                ########### Medel output\n",
    "                                file_name_parameter = save_folder + 'model_param_' + time_str + '.pth' \n",
    "                                file_name_whole = save_folder + 'model_whole_' + time_str + '.pth'\n",
    "                                file_name_mean_std = save_folder + 'train_mean_std_' + time_str + '.csv'\n",
    "                                \n",
    "                                torch.save(model.state_dict(), file_name_parameter)\n",
    "                                \n",
    "                                torch.save(model, file_name_whole)\n",
    "                                \n",
    "                                pd.DataFrame({'mean_x': mean_x, 'std_x': std_x}).to_csv(file_name_mean_std, header=False, index=False)\n",
    "                                \n",
    "                                \n",
    "                                ######## Summary of results output\n",
    "                                msel = []     # l:learning\n",
    "                                mael = []\n",
    "                                maerl = []    # mean absolute error rate\n",
    "                                corl = []\n",
    "\n",
    "                                for val in range(out1_num):\n",
    "                                    msel.append( result_learning_df[ 'MSE'+out1_index[val] ].mean() )\n",
    "                                    mael.append( result_learning_df[ 'MAE'+out1_index[val] ].mean() )\n",
    "                                    maerl.append( result_learning_df[ 'MAER'+out1_index[val] ].mean() ) \n",
    "                                    corl.append( result_learning_df[ ['Estimate'+out1_index[val], 'True'+out1_index[val] ] ].corr().iloc[0, 1] )\n",
    "                                                                                    \n",
    "                                ba1l = result_learning_df['Bit_rate'].mean()\n",
    "                                em1l = result_learning_df['Exact_match'].mean()\n",
    "                                ba2l = result_learning_df['Bit_rate(replaced)'].mean()\n",
    "                                em2l = result_learning_df['Exact_match(replaced)'].mean()\n",
    "                                               \n",
    "                                mset = []      # t:test\n",
    "                                maet = []\n",
    "                                maert = []  \n",
    "                                cort = []\n",
    "\n",
    "                                for val in range(out1_num):\n",
    "                                    mset.append( result_test_df[ 'MSE'+out1_index[val] ].mean() )\n",
    "                                    maet.append( result_test_df[ 'MAE'+out1_index[val] ].mean() )\n",
    "                                    maert.append( result_test_df[ 'MAER'+out1_index[val] ].mean() )  \n",
    "                                    cort.append( result_test_df[ ['Estimate'+out1_index[val], 'True'+out1_index[val] ] ].corr().iloc[0, 1] )\n",
    "                                                                                    \n",
    "                                ba1t = result_test_df['Bit_rate'].mean()\n",
    "                                em1t = result_test_df['Exact_match'].mean()\n",
    "                                ba2t = result_test_df['Bit_rate(replaced)'].mean()\n",
    "                                em2t = result_test_df['Exact_match(replaced)'].mean()\n",
    "                                                \n",
    "                                mseL_label = []\n",
    "                                maeL_label = []\n",
    "                                maerL_label = []  \n",
    "                                corL_label = []\n",
    "                                mseT_label = []\n",
    "                                maeT_label = []\n",
    "                                maerT_label = []  \n",
    "                                corT_label = []\n",
    "                                \n",
    "                                for val in range(out1_num):\n",
    "                                    mseL_label.append( 'mseL_'+out1_index[val] )\n",
    "                                    maeL_label.append( 'maeL_'+out1_index[val] )\n",
    "                                    maerL_label.append( 'maerL_'+out1_index[val] )\n",
    "                                    corL_label.append( 'corrL_'+out1_index[val] )\n",
    "                                    mseT_label.append( 'mseT_'+out1_index[val] )\n",
    "                                    maeT_label.append( 'maeT_'+out1_index[val] )\n",
    "                                    maerT_label.append( 'maerT_'+out1_index[val] )\n",
    "                                    corT_label.append( 'corrT_'+out1_index[val] )\n",
    "                                    \n",
    "                                                \n",
    "\n",
    "                                file_name = save_folder + summary_result_filename + '.csv' \n",
    "                                headers = ['time', 'epoch', *mseL_label, *maeL_label, *maerL_label, *corL_label, \n",
    "                                           'Bit_rateRowL', 'Exact_matchRowL', 'Bit_rateRepL', 'Exact_matchRepL', \n",
    "                                           *mseT_label, *maeT_label, *maerT_label, *corT_label, \n",
    "                                           'Bit_rateRowT', 'Exact_matchRowT', 'Bit_rateRepT', 'Exact_matchRepT', 'val_loss', \n",
    "                                           'dataset_type', 'f_s_on',\n",
    "                                           'f_deg_on', 'f_prob_on', 'class_num', 'input_norm', 'inverse_T', \n",
    "                                           'closeness', 'betweenness', 'clustering', 'optimizer', 'lr', 'wei_decay', \n",
    "                                           'out1_num', 'out1_weight', 'class_weight', 'patience', 'loss2_coef', 'batch', \n",
    "                                           'test_shitei', 'random_num', 'learning_data_ratio', 'val_data_ratio', \n",
    "                                           'augmentation', 'aug_rate', 'over_sample', 'auto_bin', 'os_bin', 'os_fluc', \n",
    "                                           'time', 'convolution', 'other', 'model', 'drop_p', 'orig_data_num', 'trainset_num', 'valset_num', 'testset_num']\n",
    "\n",
    "                                data_to_save = [time_str, epoch, *msel, *mael, *maerl, *corl, ba1l, em1l, ba2l, em2l, \n",
    "                                                *mset, *maet, *maert, *cort, ba1t, em1t, ba2t, em2t, val_loss.item(), \n",
    "                                                dataset_type, feature_s_on, feature_degree_on, feature_prob_on, \n",
    "                                                class_num, input_normalization, inverse_T, closeness, betweenness, clustering, \n",
    "                                                optimizer_select, learning_rate, weight_decay_val, out1_num, str(out1_weight), \n",
    "                                                str(weights), patience_val, loss2_coef, \n",
    "                                                learning_batch_size,\n",
    "                                                str(test_data_shitei), str(random_data_num), learning_data_ratio, validation_data_ratio, \n",
    "                                                adding_split_brain_data, add_split_brain_data_rate, over_sampling, auto_bins,\n",
    "                                                str(np.round(bins, 3)), os_fluctuation,\n",
    "                                                time_str, convolution_type, other_network_type, str(model).replace('\\n', '; '), drop_rate,\n",
    "                                                len(data_list), len(train_dataset), len(validation_dataset), len(test_dataset)]\n",
    "\n",
    "\n",
    "                                try:\n",
    "                                    if not os.path.exists(file_name):\n",
    "                                        with open(file_name, 'w', newline='', encoding='utf-8') as f:   \n",
    "                                            writer = csv.writer(f)\n",
    "                                            writer.writerow(headers)\n",
    "                                            writer.writerow(data_to_save)\n",
    "                                            initial_output_flag = 0\n",
    "                                    else:\n",
    "                                        with open(file_name, 'a', newline='', encoding='utf-8') as f:\n",
    "                                            writer = csv.writer(f)\n",
    "                                            \n",
    "                                            if initial_output_flag == 1:   \n",
    "                                                writer.writerow(headers)\n",
    "                                                initial_output_flag = 0\n",
    "\n",
    "                                            writer.writerow(data_to_save)\n",
    "                                        \n",
    "                                except IOError:\n",
    "                                    new_file_name = f'{save_folder+\"TMP_\"+summary_result_filename+\"_\"+time_str+\".csv\"}'\n",
    "                                    with open(new_file_name, 'w', newline='', encoding='shift-jis') as f:\n",
    "                                        writer = csv.writer(f)\n",
    "                                        writer.writerow(headers)\n",
    "                                        writer.writerow(data_to_save)\n",
    "\n",
    "\n",
    "\n",
    "                                ######## output of loss_history\n",
    "                                file_name = save_folder + 'history_' + time_str + '.xlsx' \n",
    "\n",
    "                                loss_history_df = pd.DataFrame( loss_history, \n",
    "                                                               columns=['train_total_loss', 'train_loss1', 'train_loss2',\n",
    "                                                                        'val_total_loss', 'val_loss1', 'val_loss2'])\n",
    "\n",
    "                                loss_history_df.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiuOkExvxY8a6qqZHd5R67",
   "collapsed_sections": [],
   "name": "20220601-GNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "cu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
